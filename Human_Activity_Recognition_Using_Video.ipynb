{
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "name": "‚≠êÔ∏èüì¢Human_Activity_Recognition_Using_Video‚≠êÔ∏è",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7483630,
          "sourceType": "datasetVersion",
          "datasetId": 4356509
        }
      ],
      "dockerImageVersionId": 30636,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkp3012/assignment/blob/main/Human_Activity_Recognition_Using_Video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'human-activity-recognition-ucf50-video-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4356509%2F7483630%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240503%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240503T073535Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4fd1521fc2bae21eb9a3dc59c7589f42b36ebe69f39f55cc7378b0fdfa6f9d8374b9707c85f44a3e07985b27dd87c2bf2e5ce7f4f11929cabad9be6d1665089464e5572aa651c3fc2d5598fec703850d7925fef17709f3334c2e18f9a3a8ee857f983fd9f29b6769c0309e71711135ca6b236193808aacceb80566306bdde533c65a7726f23038d3daa1b1f7a744721b0d601654b7919478c67b6e3901e4623ff2b73d35b433afbaff856edeb2f345473cebcaba2c1fc9fee71051183fe795d9a77e9859aacbb6da21b26bad600607ec8d399f09166780e798584ffdd3ba2b594ebc5d2e03a14fd721a8c73449d35fd0a8865b1abd162dd37b25d175a7f55188'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "7TV4q_OuYVqa"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <h1><center><b>Human Activity Recognition From Videos</b></center></h1>\n",
        "\n",
        "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fuser-images.githubusercontent.com%2F46052843%2F191772606-f599c270-97b2-43a9-ab91-563b4dc6a882.png&f=1&nofb=1&ipt=70eef176a1e6e40ecb7676201db2d9717ab69290ba8dae9946d027b17e7c4bea&ipo=images\" alt=center>\n",
        "\n",
        "\n",
        "# **Steps**\n",
        "\n",
        "### 1. Import require library\n",
        "### 2. Summarize the dataset\n",
        "### 3. Visualize the dataset with labels\n",
        "### 4. Preprocess the dataset\n",
        "### 5. Split the dataset into train and test\n",
        "### 6. Design and Develop dnn architecture and train the model\n",
        "### 7. Testing the model\n",
        "### 8. Report the result\n",
        "### 9. Predict the output"
      ],
      "metadata": {
        "id": "9kmqiKXEYVqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import the required libraries"
      ],
      "metadata": {
        "id": "RiAMJx7tr5-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tensorflow opencv-contrib-python pydot moviepy pafy"
      ],
      "metadata": {
        "id": "Ft-u2b1iUQpg",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:44:10.415989Z",
          "iopub.execute_input": "2024-01-26T06:44:10.416643Z",
          "iopub.status.idle": "2024-01-26T06:44:22.370586Z",
          "shell.execute_reply.started": "2024-01-26T06:44:10.416604Z",
          "shell.execute_reply": "2024-01-26T06:44:22.369418Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries.\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from moviepy.editor import *\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "X3AdbpZFCRR0",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:44:22.372894Z",
          "iopub.execute_input": "2024-01-26T06:44:22.373239Z",
          "iopub.status.idle": "2024-01-26T06:44:22.383929Z",
          "shell.execute_reply.started": "2024-01-26T06:44:22.373211Z",
          "shell.execute_reply": "2024-01-26T06:44:22.382986Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Seeds to get consistent results on every execution\n",
        "seed_constant = 42\n",
        "np.random.seed(seed_constant)\n",
        "random.seed(seed_constant)\n",
        "tf.random.set_seed(seed_constant)"
      ],
      "metadata": {
        "id": "9AFIAq0Q0VwG",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:44:22.385252Z",
          "iopub.execute_input": "2024-01-26T06:44:22.385524Z",
          "iopub.status.idle": "2024-01-26T06:44:22.513205Z",
          "shell.execute_reply.started": "2024-01-26T06:44:22.3855Z",
          "shell.execute_reply": "2024-01-26T06:44:22.512235Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Acquisition\n",
        "\n",
        "\n",
        "Dataset url: https://www.crcv.ucf.edu/data/UCF50.php\n",
        "\n",
        "Dataset summary:\n",
        "\n",
        "\n",
        "*   **`50`** Action Categories\n",
        "\n",
        "*   **`25`** Groups of Videos per Action Category\n",
        "\n",
        "*   **`133`** Average Videos per Action Category\n",
        "\n",
        "*   **`199`** Average Number of Frames per Video\n",
        "\n",
        "*   **`320`** Average Frames Width per Video\n",
        "\n",
        "*   **`240`** Average Frames Height per Video\n",
        "\n",
        "*   **`26`** Average Frames Per Seconds per Video\n",
        "\n"
      ],
      "metadata": {
        "id": "aLMChkaXYVql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the data with its label"
      ],
      "metadata": {
        "id": "5Uh9KFYfYVqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_classes_names = os.listdir('/kaggle/input/human-activity-recognition-ucf50-video-dataset/UCF50')\n",
        "print(all_classes_names)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-26T06:44:22.519396Z",
          "iopub.execute_input": "2024-01-26T06:44:22.519733Z",
          "iopub.status.idle": "2024-01-26T06:44:22.535002Z",
          "shell.execute_reply.started": "2024-01-26T06:44:22.519706Z",
          "shell.execute_reply": "2024-01-26T06:44:22.534143Z"
        },
        "trusted": true,
        "id": "e3S0QdV_YVqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20, 20))\n",
        "random_range = random.sample(range(len(all_classes_names)),16)\n",
        "\n",
        "for counter, random_index in enumerate(random_range, 1):\n",
        "    selected_class_Name = all_classes_names[random_index]\n",
        "    video_files_names_list = os.listdir(f'/kaggle/input/human-activity-recognition-ucf50-video-dataset/UCF50/{selected_class_Name}')\n",
        "    selected_video_file_name = random.choice(video_files_names_list)\n",
        "    video_reader = cv2.VideoCapture(f'/kaggle/input/human-activity-recognition-ucf50-video-dataset/UCF50/{selected_class_Name}/{selected_video_file_name}')\n",
        "    _, bgr_frame = video_reader.read()\n",
        "    video_reader.release()\n",
        "    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n",
        "    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 3)\n",
        "    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')"
      ],
      "metadata": {
        "id": "oQnKb7fbC2aq",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:44:22.536025Z",
          "iopub.execute_input": "2024-01-26T06:44:22.536319Z",
          "iopub.status.idle": "2024-01-26T06:44:23.977519Z",
          "shell.execute_reply.started": "2024-01-26T06:44:22.536295Z",
          "shell.execute_reply": "2024-01-26T06:44:23.976611Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Preparation\n"
      ],
      "metadata": {
        "id": "Mxsl74v8Ud2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n",
        "SEQUENCE_LENGTH = 20\n",
        "DATASET_DIR = \"/kaggle/input/human-activity-recognition-ucf50-video-dataset/UCF50\"\n",
        "# I am trying specific 5 classes (For system constrain)\n",
        "CLASSES_LIST = ['HorseRace', 'VolleyballSpiking', 'Biking', 'TaiChi', 'Punch', 'BreastStroke', 'Billiards', 'PoleVault', 'ThrowDiscus', 'BaseballPitch', 'HorseRiding', 'Mixing', 'HighJump', 'Skijet', 'SkateBoarding', 'MilitaryParade', 'Fencing', 'JugglingBalls', 'Swing', 'RockClimbingIndoor', 'SalsaSpin', 'PlayingTabla', 'Rowing', 'BenchPress', 'PushUps', 'Nunchucks', 'PlayingViolin']\n",
        "print(\"My specific classes count:\",len(CLASSES_LIST))"
      ],
      "metadata": {
        "id": "gb4Fv7Ag-kwb",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:44:23.978839Z",
          "iopub.execute_input": "2024-01-26T06:44:23.979683Z",
          "iopub.status.idle": "2024-01-26T06:44:23.985436Z",
          "shell.execute_reply.started": "2024-01-26T06:44:23.979646Z",
          "shell.execute_reply": "2024-01-26T06:44:23.984367Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Helper function**\n",
        "\n",
        "1. Extract and normalize the dataset\n",
        "2. Create a Function for Dataset Creation\n",
        "3. Plot Model's Loss & Accuracy Curves"
      ],
      "metadata": {
        "id": "y9EnAZdHYVqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frames_extraction(video_path):\n",
        "    '''\n",
        "    This function will extract the required frames from a video after resizing and normalizing them.\n",
        "    Args:\n",
        "        video_path: The path of the video in the disk, whose frames are to be extracted.\n",
        "    Returns:\n",
        "        frames_list: A list containing the resized and normalized frames of the video.\n",
        "    '''\n",
        "    frames_list = []\n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
        "    for frame_counter in range(SEQUENCE_LENGTH):\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "        success, frame = video_reader.read()\n",
        "        if not success:\n",
        "            break\n",
        "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "        normalized_frame = resized_frame / 255\n",
        "        frames_list.append(normalized_frame)\n",
        "    video_reader.release()\n",
        "\n",
        "    return frames_list"
      ],
      "metadata": {
        "id": "zBu224OG-szz",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:44:23.987088Z",
          "iopub.execute_input": "2024-01-26T06:44:23.987767Z",
          "iopub.status.idle": "2024-01-26T06:44:23.996071Z",
          "shell.execute_reply.started": "2024-01-26T06:44:23.987734Z",
          "shell.execute_reply": "2024-01-26T06:44:23.995093Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset():\n",
        "    '''\n",
        "    This function will extract the data of the selected classes and create the required dataset.\n",
        "    Returns:\n",
        "        features:          A list containing the extracted frames of the videos.\n",
        "        labels:            A list containing the indexes of the classes associated with the videos.\n",
        "        video_files_paths: A list containing the paths of the videos in the disk.\n",
        "    '''\n",
        "    features = []\n",
        "    labels = []\n",
        "    video_files_paths = []\n",
        "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
        "        print(f'Extracting Data of Class: {class_name}')\n",
        "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
        "        for file_name in files_list:\n",
        "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
        "            frames = frames_extraction(video_file_path)\n",
        "            if len(frames) == SEQUENCE_LENGTH:\n",
        "                features.append(frames)\n",
        "                labels.append(class_index)\n",
        "                video_files_paths.append(video_file_path)\n",
        "    features = np.asarray(features)\n",
        "    labels = np.array(labels)\n",
        "    return features, labels, video_files_paths"
      ],
      "metadata": {
        "id": "j6MLp9PpDJ4-",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:44:23.997527Z",
          "iopub.execute_input": "2024-01-26T06:44:23.997991Z",
          "iopub.status.idle": "2024-01-26T06:44:24.009227Z",
          "shell.execute_reply.started": "2024-01-26T06:44:23.99792Z",
          "shell.execute_reply": "2024-01-26T06:44:24.008318Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n",
        "    '''\n",
        "    This function will plot the metrics passed to it in a graph.\n",
        "    Args:\n",
        "        model_training_history: A history object containing a record of training and validation\n",
        "                                loss values and metrics values at successive epochs\n",
        "        metric_name_1:          The name of the first metric that needs to be plotted in the graph.\n",
        "        metric_name_2:          The name of the second metric that needs to be plotted in the graph.\n",
        "        plot_name:              The title of the graph.\n",
        "    '''\n",
        "\n",
        "    metric_value_1 = model_training_history.history[metric_name_1]\n",
        "    metric_value_2 = model_training_history.history[metric_name_2]\n",
        "    epochs = range(len(metric_value_1))\n",
        "    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n",
        "    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n",
        "    plt.title(str(plot_name))\n",
        "    plt.legend()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-26T06:44:24.010519Z",
          "iopub.execute_input": "2024-01-26T06:44:24.011228Z",
          "iopub.status.idle": "2024-01-26T06:44:24.020714Z",
          "shell.execute_reply.started": "2024-01-26T06:44:24.011197Z",
          "shell.execute_reply": "2024-01-26T06:44:24.019731Z"
        },
        "trusted": true,
        "id": "Sm41XNRhYVqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create the dataset**"
      ],
      "metadata": {
        "id": "DE5UOb1tYVqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels, video_files_paths = create_dataset()"
      ],
      "metadata": {
        "id": "ckAYgSriJN_R",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:44:24.025305Z",
          "iopub.execute_input": "2024-01-26T06:44:24.025869Z",
          "iopub.status.idle": "2024-01-26T06:45:08.732811Z",
          "shell.execute_reply.started": "2024-01-26T06:44:24.025835Z",
          "shell.execute_reply": "2024-01-26T06:45:08.731978Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using Keras's to_categorical method to convert labels into one-hot-encoded vectors**\n"
      ],
      "metadata": {
        "id": "goRGrc8_0Usb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoded_labels = to_categorical(labels)"
      ],
      "metadata": {
        "id": "KfcpuV6_Bd_T",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:08.734132Z",
          "iopub.execute_input": "2024-01-26T06:45:08.73489Z",
          "iopub.status.idle": "2024-01-26T06:45:08.739433Z",
          "shell.execute_reply.started": "2024-01-26T06:45:08.734852Z",
          "shell.execute_reply": "2024-01-26T06:45:08.738501Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Split the Data into Train and Test Set**"
      ],
      "metadata": {
        "id": "PzRt7pP-0j5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels,\n",
        "                                                                            test_size = 0.20, shuffle = True,\n",
        "                                                                            random_state = seed_constant)"
      ],
      "metadata": {
        "id": "UhkQjq1JJSO2",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:08.741015Z",
          "iopub.execute_input": "2024-01-26T06:45:08.741444Z",
          "iopub.status.idle": "2024-01-26T06:45:09.085355Z",
          "shell.execute_reply.started": "2024-01-26T06:45:08.741406Z",
          "shell.execute_reply": "2024-01-26T06:45:09.084521Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Deep Neural Network Architecture"
      ],
      "metadata": {
        "id": "5_f7EN8_YVqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Design the architecture that you will be using LRCN\n",
        "\n",
        "\n",
        "we will implement an approach known as the Long-term Recurrent Convolutional Network (LRCN), that combines CNN and LSTM layers in a single model. The Convolutional layers are used for spatial feature extraction from the frames, and the extracted spatial features are fed to LSTM layers at each time-steps for temporal sequence modeling. This way the network learns spatiotemporal features directly in an end-to-end training, resulting in a robust model.\n",
        "\n",
        "<center>\n",
        "<img src='https://i.stack.imgur.com/FuxJA.png'>\n",
        "</center>\n",
        "\n",
        "As part of the solution, we will also put TimeDistributed wrapper layer to use, which allows applying the same layer to every frame of the video independently. This makes a layer (around which it is wrapped) capable of taking input of shape no_of_frames, width, height, num_of_channels if originally the layer's input shape was width, height, num_of_channels. This is very beneficial as it allows us to input the whole video into the model in a single shot.\n"
      ],
      "metadata": {
        "id": "clp7BFVDBNxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the model\n",
        "\n",
        "def create_LRCN_model():\n",
        "    '''\n",
        "    This function will construct the required LRCN model.\n",
        "    Returns:\n",
        "        model: It is the required constructed LRCN model.\n",
        "    '''\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),\n",
        "                              input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
        "\n",
        "    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n",
        "    model.add(TimeDistributed(Dropout(0.25)))\n",
        "\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n",
        "    model.add(TimeDistributed(Dropout(0.25)))\n",
        "\n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Dropout(0.25)))\n",
        "\n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Dropout(0.25)))\n",
        "\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "    model.add(LSTM(32))\n",
        "\n",
        "    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n",
        "\n",
        "    print(\"Report of Model layer & parameters:\")\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "HKM766mB1fIv",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:09.086556Z",
          "iopub.execute_input": "2024-01-26T06:45:09.086858Z",
          "iopub.status.idle": "2024-01-26T06:45:09.09825Z",
          "shell.execute_reply.started": "2024-01-26T06:45:09.086833Z",
          "shell.execute_reply": "2024-01-26T06:45:09.097378Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the required LRCN model.\n",
        "LRCN_model = create_LRCN_model()\n",
        "print(\"Model Created Successfully\")"
      ],
      "metadata": {
        "id": "ZJjC2VA2JkqD",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:09.09937Z",
          "iopub.execute_input": "2024-01-26T06:45:09.099694Z",
          "iopub.status.idle": "2024-01-26T06:45:10.060255Z",
          "shell.execute_reply.started": "2024-01-26T06:45:09.099667Z",
          "shell.execute_reply": "2024-01-26T06:45:10.059371Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Training the model"
      ],
      "metadata": {
        "id": "wsDrzxCDA5lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n",
        "LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
        "LRCN_model_training_history = LRCN_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 8 ,\n",
        "                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])"
      ],
      "metadata": {
        "id": "mtrQGXc8A5lY",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:10.061646Z",
          "iopub.execute_input": "2024-01-26T06:45:10.062308Z",
          "iopub.status.idle": "2024-01-26T06:45:18.50016Z",
          "shell.execute_reply.started": "2024-01-26T06:45:10.062273Z",
          "shell.execute_reply": "2024-01-26T06:45:18.499114Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Test the model\n"
      ],
      "metadata": {
        "id": "tMepQus5A5la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_evaluation_history = LRCN_model.evaluate(features_test, labels_test)"
      ],
      "metadata": {
        "id": "ZwSYdhEFA5lb",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:18.502347Z",
          "iopub.execute_input": "2024-01-26T06:45:18.503454Z",
          "iopub.status.idle": "2024-01-26T06:45:19.321313Z",
          "shell.execute_reply.started": "2024-01-26T06:45:18.503415Z",
          "shell.execute_reply": "2024-01-26T06:45:19.320401Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Report the result"
      ],
      "metadata": {
        "id": "bDs61KA-A5lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss\n",
        "plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')"
      ],
      "metadata": {
        "id": "nvAgcD_FA5ld",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:19.322791Z",
          "iopub.execute_input": "2024-01-26T06:45:19.323201Z",
          "iopub.status.idle": "2024-01-26T06:45:19.666326Z",
          "shell.execute_reply.started": "2024-01-26T06:45:19.323168Z",
          "shell.execute_reply": "2024-01-26T06:45:19.665379Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy\n",
        "plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')"
      ],
      "metadata": {
        "id": "YZHXzXJZA5le",
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:19.667626Z",
          "iopub.execute_input": "2024-01-26T06:45:19.668111Z",
          "iopub.status.idle": "2024-01-26T06:45:20.017752Z",
          "shell.execute_reply.started": "2024-01-26T06:45:19.668075Z",
          "shell.execute_reply": "2024-01-26T06:45:20.016795Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_evaluation_history = LRCN_model.evaluate(features_test, labels_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:20.019025Z",
          "iopub.execute_input": "2024-01-26T06:45:20.019332Z",
          "iopub.status.idle": "2024-01-26T06:45:20.3047Z",
          "shell.execute_reply.started": "2024-01-26T06:45:20.019305Z",
          "shell.execute_reply": "2024-01-26T06:45:20.303923Z"
        },
        "trusted": true,
        "id": "eb7UdodNYVqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "#prediction\n",
        "predictions = LRCN_model.predict(features_test)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(labels_test, axis=1)\n",
        "\n",
        "# confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:20.306005Z",
          "iopub.execute_input": "2024-01-26T06:45:20.306323Z",
          "iopub.status.idle": "2024-01-26T06:45:21.097363Z",
          "shell.execute_reply.started": "2024-01-26T06:45:20.306296Z",
          "shell.execute_reply": "2024-01-26T06:45:21.096266Z"
        },
        "trusted": true,
        "id": "JV0DSwn-YVqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "CLASSES_LIST = ['HorseRace', 'VolleyballSpiking', 'Biking', 'TaiChi', 'Punch', 'BreastStroke', 'Billiards', 'PoleVault', 'ThrowDiscus', 'BaseballPitch', 'HorseRiding', 'Mixing', 'HighJump', 'Skijet', 'SkateBoarding', 'MilitaryParade', 'Fencing', 'JugglingBalls', 'Swing', 'RockClimbingIndoor', 'SalsaSpin', 'PlayingTabla', 'Rowing', 'BenchPress', 'PushUps', 'Nunchucks', 'PlayingViolin']\n",
        "\n",
        "# Assuming you already have predictions, true_labels, etc.\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES_LIST, yticklabels=CLASSES_LIST)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:21.09884Z",
          "iopub.execute_input": "2024-01-26T06:45:21.099424Z",
          "iopub.status.idle": "2024-01-26T06:45:21.416388Z",
          "shell.execute_reply.started": "2024-01-26T06:45:21.099386Z",
          "shell.execute_reply": "2024-01-26T06:45:21.415397Z"
        },
        "trusted": true,
        "id": "7fHTaTfuYVq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the Model"
      ],
      "metadata": {
        "id": "nAIJvq1QYVq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n",
        "\n",
        "date_time_format = '%Y_%m_%d__%H_%M_%S'\n",
        "current_date_time_dt = dt.datetime.now()\n",
        "current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n",
        "\n",
        "model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n",
        "\n",
        "LRCN_model.save(model_file_name)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:21.417566Z",
          "iopub.execute_input": "2024-01-26T06:45:21.417847Z",
          "iopub.status.idle": "2024-01-26T06:45:21.464253Z",
          "shell.execute_reply.started": "2024-01-26T06:45:21.417821Z",
          "shell.execute_reply": "2024-01-26T06:45:21.463558Z"
        },
        "trusted": true,
        "id": "P91qHs4-YVrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction\n"
      ],
      "metadata": {
        "id": "6cXX6Ci0YVrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):\n",
        "    '''\n",
        "    Args:\n",
        "    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n",
        "    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.\n",
        "    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.\n",
        "    '''\n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'),\n",
        "                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n",
        "    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
        "    predicted_class_name = ''\n",
        "    while video_reader.isOpened():\n",
        "        ok, frame = video_reader.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "        normalized_frame = resized_frame / 255\n",
        "        frames_queue.append(normalized_frame)\n",
        "        if len(frames_queue) == SEQUENCE_LENGTH:\n",
        "            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]\n",
        "            predicted_label = np.argmax(predicted_labels_probabilities)\n",
        "            print(predicted_label)\n",
        "            predicted_class_name = CLASSES_LIST[predicted_label]\n",
        "        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "        video_writer.write(frame)\n",
        "    video_reader.release()\n",
        "    video_writer.release()"
      ],
      "metadata": {
        "id": "s-ytt8bLwUqt",
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:21.465285Z",
          "iopub.execute_input": "2024-01-26T06:45:21.465557Z",
          "iopub.status.idle": "2024-01-26T06:45:21.475132Z",
          "shell.execute_reply.started": "2024-01-26T06:45:21.465533Z",
          "shell.execute_reply": "2024-01-26T06:45:21.474187Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_video_file_path = f'Output-SeqLen{SEQUENCE_LENGTH}.mp4'\n",
        "\n",
        "input_video_file_path = r\"/kaggle/input/human-activity-recognition-ucf50-video-dataset/UCF50/HorseRiding/v_HorseRiding_g01_c01.avi\"\n",
        "\n",
        "# Perform Action Recognition on the Test Video.\n",
        "predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-26T06:45:21.476636Z",
          "iopub.execute_input": "2024-01-26T06:45:21.476909Z",
          "iopub.status.idle": "2024-01-26T06:45:34.187315Z",
          "shell.execute_reply.started": "2024-01-26T06:45:21.47688Z",
          "shell.execute_reply": "2024-01-26T06:45:34.18637Z"
        },
        "trusted": true,
        "id": "PTmU4sfrYVrT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}